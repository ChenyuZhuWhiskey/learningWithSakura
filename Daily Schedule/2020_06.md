# 20200607之前的汇总

- the art of softward security assessment 翻译完第一，二章，第五章memory corruption翻译了一点（stack overflow）
- primer c++ 看到了13章，其中跳过了第8章IO库，第9章顺序容器，第10章泛型算法，第11章关联容器。了解了右值引用和左值引用的区别，主要的应用表现在拷贝构造`Ctor(&__x)`和移动构造`Ctor(&&__x)`上，以及`std::move()`将左值引用转换右值引用。
- SGI stl抄写完了分配器和迭代器，容器写完`vector`和`list`
- stl看侯捷视频，看完了分配器，迭代器，容器看完了`list`和`vector`
  - 理解了default allocator（`alloc`）的原·理（用`operator::new`,也就是`malloc`包装）
  - 理解了迭代器的五种类型，容器所使用的是`_InputIter`，`ostream`使用的是`_OutputIter`，继承关系`input -> forward -> bidirectional -> random_access`
  - 理解了`iter`是泛化的指针，但不同类型`iter`允许的操作级别不同。`forward`只能向前，`bidirectional`能向前向后，`random_access`能任意加减（指向连续空间）
  - vector是拥有连续内存空间的容器，迭代器类型为`random_access`，SGI stl实现结构如下：
    `_Vector_base --protected--> vector`
      其中`_Vector_base`提供指向首元素，末元素+1， `end_of_storage`的三根指针，提供alloc和dealloc的接口
      vector提供更多接口，实现了内存动态增长，原理就是`_M_finish` == `_M_end_of_storage`时重新分配一段size*2的内存空间，将储存的对象拷贝到新区域，将旧区域destroy。
  - list是双向链表的容器，内存空间中的分布实质上是环状的（末尾`next`指向首位），迭代器类型为bidirectional， SGI实现结构如下：
    `_List_node_base --public--> _List_node`
    `_List_iterator_base --public--> _List_iterator`
    `_List_base --protected--> list`
      其中`_List_node_base`储存了指向前和向后的两根指针， `_List_node`继承`_List_node_base`的指针并加了 `_M_data`.
      `_List_iterator_base` 提供了`iter`向前和向后的接口，==和!=的operator，`_List_iterator`给出了其他类似指针的操作：`*，->， ++, ++(int), --, --(int)`
      `_List_base`拥有`protected` `_List_node`成员，给出了创建删除`node`的接口，`list`提供更多接口和基础算法
- 了解了一些模板元编程利用偏特化做类型推导的技巧，stl里主要表现为：
  - 推导一个类型是否为单字节（给出`char`等的偏特化版本）
  - 推导一个类型是否为integer（将`char，int`等设为 `__true_type`，做偏特化时额外加一个参数 `__true_type`)
  - 推导一个类型是否时POD(给出`_Tp*`的偏特化版本，将POD全部设为`__true_type`)

- `uninitialized_xxx` 和` copy/fill`的区别在于如果传入的是对象，那么`uninitialized_xxx`调用的是allocator的`_Construct()`函数，即拷贝构造。而`copy/fill`调用的是拷贝赋值，拷贝构造是将一个对象拷贝到对未初始化的空间中形成新对象，而拷贝赋值是将一个对象的成员值复制到对一个已经初始化的对象中。

# 20200607

## 树

STL的Sequence Container已经基本实现了(Deque, Array, Stack)等基本上都是vector和list的变种，这里暂时略过，有时间补上。

在开始Associative Container之前，没有系统学习过数据结构的我需要先补充数据结构知识，现在开始看Tree：

### Binary Search

我们知道，binary search的时间复杂度是O(log n)，而用遍历搜索是O(n)，但要想实现binary search，被搜索的序列必须是sorted in ordered，如果对于一个定义了比较算符的sequence，我们也想使用binary search，那就可以利用binary search的思想构造这样一种数据结构：二叉搜索树

```C++
struct BSTree{
  _Data data;
  BSTree* left;
  BSTree* right;
};
```

其中满足性质：`left < data, right > data`，那么在搜索节点的时候，就可以使用Binary Search一样的方法了：

```C++
static BSTree* Search(BSTree* __tree, _Data& __data){
    if(!__tree){
        return nullptr;
    }
    if(__data.equal(__tree.data)){
        return __tree;
    }
    if(__data > __tree.data){
        return Search(__tree->right, __data);
    }else{
        return Search(__tree->left, __data);
    }
}
```

#### 复杂度

最好的情况下，`BSTree`的搜索复杂度为O(log N)，其中N是节点数，但最坏的情况也是存在的，比如所有数据全部集中在了`left`或者`right`，此时`BSTree`就退化成了有序的链表，搜索时复杂度为O(N).

### B Tree

为了让一颗树不造成最坏情况，即让树深度尽量小，我们对树有这样一种改进：B Tree。也就是让一个节点能够存储L个数据，并让这个节点能够连接L+1个子节点，这样每个数据都对应了一个left和right，在形式上每个节点的数据都保留了`BSTree`中`left < data, right > data`。

### Rotation

除了使用B Tree，还可以对`BSTree`定义`leftRotate`和`rightRotate`操作。`leftRotate`就是把这个节点和`__tree.right`交换位置，让父节点指向它，然后它再移到`__tree.right`的`left`上，通过有限次数的`rotate`操作，可以将一个`BSTree`变成最佳状态。

## 南京大学计算机基础

### 越界访问与缓冲区溢出攻击

所谓缓冲区就是指程序运行时使用的内存空间。对于运行时栈(runtime stack)来说，一般在调用一个函数时，主函数会将运行完这个函数的下一个运行code的地址push到栈顶，然后push原来BP指向地址到栈顶，随后让BP，SP指向此位置构造新的函数调用栈(call stack)。假如我们在call stack中运行一个数组或者指针进行越界操作，那么攻击者可以利用这个漏洞将返回地址更改为自己预先准备好的攻击程序地址，随后运行自己的任意代码实施攻击。

### x86-64指令

x86-64指令和IA-32指令有两处主要不同：

- x86-64将寄存器扩充到了64位，`rax(63)->eax(32)->ax(16)->ah(8)->al(8)`
- 使用6个寄存器在函数调用时储存变量，当函数参数小于等于6时(basic type or pointer)，将使用寄存器储存变量而不是push到栈中

另外，x86-64架构的`long long`占64位，32位机器`long long`是32位。算术逻辑运算指令也相对32位指令有改变。

### 可执行文件生成

C语言在生成可执行文件时经历如下步骤:

- 预处理，主要是展开预编译指令(`#ifndef`等)，插入头文件，删除注释
- 编译为汇编指令
- 汇编指令编译为二进制指令
- 链接为可执行文件

其中链接过程就是把多个二进制指令文件合并成能够执行的二进制文件，比如在`main()`函数里调用了`printf()`，链接时就会将编译时给出的`printf()`地址找到相应的code，然后拼在一起。

每一个源代码文件编译生成的二进制文件都有三个区域：代码区`.text`，已定义的数据区`.data`，还有未定义的数据区`.bss`（placeholder），链接器就是负责将这些区域的数据一一对应合并，然后放在虚拟内存地址空间的过程。

# 20200608

## 红黑树

### 定义与性质

红黑树可以这么理解：`2-3Tree + BTree`.

对于一个`2-3Tree`，我们可以用以下约定将它转化为`BSTree`，这样的`BSTree`其实就是`RBTree`（`RBTree`本质上就是高度平衡的`BSTree`）：split一个node后，让右者（大的）成为父节点

![BT_to_RBT](https://chenyu-learning-with-sakura-img-1302348848.cos.ap-nanjing.myqcloud.com/20200608/BTree_to_RBT.png)

同时，将split后的边标为红色，其余为黑色，这个二叉树就是红黑树。

经过一些简单的证明，可以得到红黑树有以下的性质：

- 一个节点不可能有两个红色的边
- 根节点到叶的每一条路径黑色边的数量都相等

性质一是显然的，假如一个节点有两个红色的边，对应的`2-3Tree`就可能是这样：

![RBTImp](https://chenyu-learning-with-sakura-img-1302348848.cos.ap-nanjing.myqcloud.com/20200608/RBTree_imba_case.png)

显然违反了`2-3Tree`的定义。

性质二也是显然的，`2-3Tree`是`B Tree`的一种，`BTree`的定义就是它必须是平衡树(root到每个leaf的路径长度相同)，在split时只是产生了新的红色的边，显然每个路径的黑色边是相同的。

根据性质1和性质2，还可以得到推论：

- 红黑树root到leaf最大路径长度是`2N+1`，其中`N`是黑色边的数量

证明：

- 根据性质1，一个节点不可能有两条红色的边，那么一条路径上红色边最多的情形就是红黑相间，此时红色边最多为`N+1`个，于是最大路径不超过`N+(N+1) = 2N+1`，证毕。

根据推论，我们可以得到红黑树的搜索算法时间复杂度为O(log n).

### 插入算法

在定义了红黑树，给出红黑树性质之后，我们要构造红黑树的插入节点算法，其实就是把`2-3Tree`的插入算法步骤映射为`BSTree`就行了。这里用到了昨天涉及的`rorateLeft & roatateRight`。

- 第一步：add。add会出现两种case: insert新值到左或者右(其余的case都是trivial的，不提了)

![Insert_Add](https://chenyu-learning-with-sakura-img-1302348848.cos.ap-nanjing.myqcloud.com/20200608/RBT_insert_add.png)

![Insert_Add_case2](https://chenyu-learning-with-sakura-img-1302348848.cos.ap-nanjing.myqcloud.com/20200608/RBT_insert_add_case2.png)

- 假如是case2，我们先使用`rotateRight`将它变成case1：

![Insert_rotate_case2to1](https://chenyu-learning-with-sakura-img-1302348848.cos.ap-nanjing.myqcloud.com/20200608/RBT_insert_rotate_case2tocase1.png)

- 第二步split，split在`2-2Tree`里略复杂，但在`RBTree`中的映射就是变换边的颜色flip

![Insert_split](https://chenyu-learning-with-sakura-img-1302348848.cos.ap-nanjing.myqcloud.com/20200608/RBT_insert_splite.png)



- 假如插入节点时没有违反红黑树的规则1，那么flip就无需进行，假如flip后父节点又违反了规则1，那么就递归性地执行前面步骤。

### 吐槽

c++的std 红黑树没用递归写法，导致算法逻辑及其复杂，不知道开发人员是怎么想的。

## Hash Table

hash table追求的就是检索时算法复杂度为O(1). 什么时候检索会有常量复杂度？对一个数组输入key的时候。hash table就应用了这样的思想。

hash table的设计没有什么数学，是纯粹的经验。现在我有a series of obj需要存储到一个空间中，并且在search时就要像数组输入key立即给出返回值一样，那么一种可行的方法如下：

- 给出一个hash function，根据obj的key计算出hash值
- 将obj存在hash值对应编号的空间中
- 如果有不同的obj计算出了相同的hash值，那就将这个空间作为链表，在保证链表长度足够短时，检索链表的迭代时间就可以忽略不计，当作常量处理

一种最简单的hash function(大多数hash table都是这么用的)就是将key除以table的size取余:

```C++
size_type hash_func(Obj& __obj, Bucket& __bucket){
    return __obj.key%__bucket.size()
}
```

具体实现的话，在std中，bucket就是一个`vector<*_Node>`, `_Node`是一个单链表。在判断链表是否过长时，采用一种经验式的方法：当插入的数大于bucket的size时，就认为链表已经过长，此时重新申请更大的空间，并按照哈希函数重新分配各个obj。

![HT_size](https://chenyu-learning-with-sakura-img-1302348848.cos.ap-nanjing.myqcloud.com/20200608/hashTable_size.png)

std的size是这么取的，第一个是53，接下来不断加倍，然后取最近的素数作为新size。

# 20200609

## STL functor

functor是长得像函数的对象，在stl用在算法的模板参数中：

```C++
template<class _Iter, class _Cmp>
Algorithm(_Iter __iter, _Cmp __cmp)
```

由于要模仿函数，那么它们在定义时就没有数据，而是只有一个operator，size是1.

functor一共分为三类：Arithmetic，Logic， Ralational。

## STL Adaptor

adaptor·是在原始组件基础上修改得到的东西，以用来适应(adapt)某种使用环境。adaptor有三种：

### Container Adaptor

stack，queue就是用vector，deque，list作为原始容器修改的。比如stack:

```C++
void push(data){
    data.pushback(data);
}

void pop(){
    data.pop_back();
}
```



### Iterator Adaptor

修改了一些基本迭代器（五种）得到，比如反向迭代器

### Function Adaptor

用来将函数转化为函数对象(functor)，比如一些adaptor模板类就用一个函数指针作为输入参数构造functor。



今天写完了adaptor，tree用SGI_STL的tree先凑合，不是用递归写的算法过于复杂。

# 20200610

## STL

hashtable写了一半。

## 计算机基础

到10.4.2程序头表和存储器映像。

## 翻译The art of software security assessment

第五章往前推了一点，到ESH Attack

# 20200611

## STL

- hashtable写完

# 20200612

## STL

map, set, hashmap, hashset完成

# 20200613

周六摸鱼

# 20200614

总结了一下STL的容器模型：

vector:

![stl_vector](https://chenyu-blog-img-1302348848.cos.ap-shanghai.myqcloud.com/stl_mem_diagram/stl_vector.png)

list:双向链表，首尾相连（环状）

![stl_list](https://chenyu-blog-img-1302348848.cos.ap-shanghai.myqcloud.com/stl_mem_diagram/stl_list.png)

hashtable：std::vector<*_Node>

![stl_hashtable](https://chenyu-blog-img-1302348848.cos.ap-shanghai.myqcloud.com/stl_mem_diagram/stl_hashtable.png)

tree: 红黑树，不画了

set/map：一个底层为红黑树的adaptor，set的key为value，map有key有value

hashset/hashmap：底层为hashtable的set/map。

- STL可能会出的一些错误：

  - 若自定义对象包含指针类型，使用默认拷贝构造函数（浅拷贝），则会触发UAF

  - `erase()`函数没有检查边界，误用会导致访问未初始化的内存

  - ```C++
    #include <vector>
    int main() {
        std::vector<int> test = std::vector<int>();
        test.push_back(1);
        auto iter = test.begin();
        test.erase(iter + 2);
        return 0;
    }
    ```

    使用`insert()`，`erase()`等会改变vector内部三根指针指向位置的method时，若预先cache了`begin()`，`end()`，`size()`等，在使用这些method后cache的值就不等于真正的值了，继续使用可能会导致访问到未初始化的内存

# 20200615

开始编译原理

COOL语言初探

## 正则表达式

正则表达式是定义在字符集$\Sigma$上的表达式，基本形式为五种：

- single character：`'c' = {"c"}`
- epsilon:  $\epsilon$ = `{""}`
- union: $A + B = \{a| a \in A or a \in B\}$  
- concatenation: $AB = \{ab | a\in A, b\in B\}$
- iteration: $A* = AA...A$

根据这五个基本表达式，可以构造出完整的常用正则表达式，并用于匹配code中的各种字符串

# 20200616

- Flex掌握
- 开始PA2

# 20200617

PA2做到满分（63/63）

一些细节需要注意：

- \n 和 \\\\\n (escape 接上换行符，EOF等的处理)
- string buffer overflow
- 处理string时，escaped character细节
- block comment在嵌套时，*)和**)的识别

# 20200618

写完PA2的时候发现自己是直接从正则文法和自动机上手的，前面的一些知识没有补：

## 文法的定义

$G = (V_T, V_N, P, S)$

其中：

$V_T$是终结符(terminal)集合，terminal就是语言基本符号的集合，也称为token

$V_N$是非终结符(non-terminal)集合，non-terminal就是非token的符号

$P$是产生式(Production)集合，一般形式：$\alpha \rightarrow \beta$，其中$\alpha,\beta \in V_T \cup V_N$，并且$\alpha$一定包含一个non-terminal.(其实就是文法的推导规则)

$S$是开始符号，文法中最大的语法成分，即通过production，S可以用有限次步骤推出所有满足该文法的句子。

## 语言定义

满足某个文法的所有句子集合

## 四种文法

根据production施加不同约束，可以定义有包含关系的四种文法：

- Type0: $\alpha$至少有一个non-terminal

- Type1（上下文有关文法,context-sensitive grammar, CSG）：$\forall \alpha \rightarrow \beta \in P, |\alpha|  \leqslant |\beta|$ .Type1不包含$\epsilon$产生式，因为$\alpha$non-terminal至少为1，那么$\beta$根据定义就不能是0
- Type2(上下文无关语言, context-free grammar, CFG)：production左部必须是一个non-terminal，一般形式$A\rightarrow \beta$
- Type3(正则文法，regular grammar, RG)两种形式
  - 左线性：$A \rightarrow wB | w$
  - 右线性：$A \rightarrow Bw | w$

包含关系：$0 \in 1 \in 2 \in 3$

## 分析树

根节点为S，用production形成字节点，一步步推导到每个叶子都是terminal

## 等价性

正则文法 = 分析树 = NFA = DFA

一般地，构造lexer步骤就是：正则文法->NFA->DFA

# 20200619

今天看病，鸽一天

# 20200620

## NFA到DFA转换步骤

NFA直观，DFA在计算机中容易实现(因为一个input token只会指向一个状态)，一般实现步骤就是构造NFA中每个state在get到一个token进入可能的所有state的集合作为DFA的新state，例如：

![NFA2DFA](https://chenyu-learning-with-sakura-img-1302348848.cos.ap-nanjing.myqcloud.com/20200620/NFA2DFA.png)

## 语法分析(parse)

判断一个句子是否属于该语法的步骤，一般分为自顶向下和自底向上。

top-down也叫推导(deduction).是从Root到Leaf构造分析树，从开始符号S推导出字符串w，例如：

![top-down](https://chenyu-learning-with-sakura-img-1302348848.cos.ap-nanjing.myqcloud.com/20200620/top-down-parse.png)

推导两种进行的方法：最左推导和最右推导。最左推导就是在production的RHS最左边的non-terminal开始，将它推成符合的terminal为止，再对下一个non-terminal进行推导，最右推导类似。

down-top反过来，用句子去推出S，这个过程叫归约(derivation)。

在自底向上时，也分为最左规约和最右规约。

top-down时，称最右推导为规范推导，down-top时，最左规约为规范归约。

## 自顶向下通用形式

递归下降分析，也就是递归调用non-terminal对应的production

![rdd](https://chenyu-learning-with-sakura-img-1302348848.cos.ap-nanjing.myqcloud.com/20200620/rdd.png)

弊端：

- 若存在token开头对应的多个表达式，就要一个个尝试，尝试失败要退回上一步，即回溯（backtracking）,导致效率低

例如：

```
G:
S -> aAd | aBe
A -> c
B -> B
Input: abc
```

- 推导直接左递归(immediate left recursive)，即形式为$A \rightarrow A\beta$导致无限循环

例如:

```
G:
E -> E + T
递归下降推导：
E => E + T
  => E + T + T
  => E + T + T + T
  ...
```

左递归有两种：直接左递归，即上面这种情况，间接左递归即表达式用n步推导出左递归。

消除直接左递归方法：引入新的non-terminal，转换为右递归

```
A -> Aa | beta
to
A -> beta A'
A' -> aA' | epsilon
```

消除间接左边递归，先带入推导式将它变成直接左递归，重复消除步骤即可。

消除backtracking:提取左公因子

```
A -> a beta1
A -> a beta2
to
A -> a A'
A' -> beta1 | beta 2
```

## LL1文法

若parser能够预测下一个token要对应使用哪个production就无需backtracking，这种分析模式为预测分析。

为了进行预测分析，对文法就有要求（S_文法，避免冲突）：

- 每个production的RHS以terminal为初始
- 每个non-terminal对应的candidate production的首终结符不同

从定义看出，S_文法不包含$\epsilon$ production。在A与a不匹配时，若存在$A \rightarrow \epsilon$，则可以检查a是否可以出现在A后面来决定是否使用$\epsilon$ production。

因此要构造出能使用$\epsilon$ production的文法，这就是LL1文法。

构造LL1前，要给出一些定义：

FOLLOW集：根据production的定义，A后面可能跟着的token集合

SELECT集：使用production$A \rightarrow\beta$进行推导时可以对应的token集合

在包含$\epsilon$ production时，SELECT集可以表示为：

- SELECT($A \rightarrow a\beta$) = {a}
- SELECT($A \rightarrow  \epsilon$) = FOLLOW(A)

为了使用SELECT集，就可以构造q_文法：

- production右边要么是$\epsilon$，要么以token开始
- 相同LHS的production的SELECT不相交

FIRST集：一个non-terminal能推导出的所有串首终结符集合，如果X=>*$\epsilon$，那么$\epsilon \in FIRST(X)$

LL1文法定义：

文法G是LL(1)的，当且叮当G任意两个具有相同LHS的production$A \rightarrow \alpha | \beta$满足下面条件：

- 若$\alpha,\beta$均无法推出$\epsilon$,则$FIRST( \alpha ) \cap FIRST(\beta) =  \empty $ 
- 若$\alpha,\beta$最多只有一个能推出$\epsilon$
- 若$\alpha =>* \epsilon$，则$FIRST(\alpha) \cap FOLLOW(A) = \empty$
- 若$\beta =>* \epsilon$，则$FIRST(\beta) \cap FOLLOW(A) = \empty$

LL1的定义就是为SELECT集消除歧义的。

# 20200621

- PA2完成：

![PA2](https://weekly-assignment-1302348848.cos.ap-shanghai.myqcloud.com/week5/PA2.png)

- 写了关于flex的文章  [https://chenyuzhuwhiskey.github.io/2020/06/21/flex-lexer%E5%88%86%E6%9E%90/](https://chenyuzhuwhiskey.github.io/2020/06/21/flex-lexer分析/) 

# 20200622-20200624

- 22日-23日一门期末
- 24日外地看病

# 20200625

继续学习编译原理

- SLR是通过每个项目的FOLLOW集来判断遇到移入-归约冲突时进行移入操作还是规约操作，为了解决LR(0)遇到的移入-归约冲突。它的使用前提就是这些FOLLOW集两两不相交

- LR(1)是为了解决SRL仅仅通过FOLLOW集来解决移入-归约冲突的不足。有项目：

  $A \rightarrow \alpha \cdot a \beta$

  $B \rightarrow \gamma\cdot$

  若a是B的FOLLOW集，则用产生式B归约是必要条件而非充分条件。LR(1)通过向前看一个展望符构造项目集闭包的自动机来解决移入-归约冲突。LR(1)的使用条件是要求文法是LR(1)的，几乎所有context-free grammar都是LR1的。

- LALR文法主要是为了解决LR(1)劈裂LR(0)产生的状态数过于庞大的问题，主要通过合并LR(1)的同心集实现，使用前提是合并后不产生归约-归约冲突。

# 20200626

- 看了Bison的文档，大概了解了怎么给算符消除歧义（left/right associate），文法格式等等
- 读了一点Bison生成的parser源代码

# 20200627

- 根据Bison parser源代码的分析，写了文章: [https://chenyuzhuwhiskey.github.io/2020/06/27/bison-parser%E6%B7%B1%E5%85%A5%E5%88%86%E6%9E%90/](https://chenyuzhuwhiskey.github.io/2020/06/27/bison-parser深入分析/) 

- 读了COOL的abstract syntax tree代码，大致就分了三个类：

  - nil_list
  - single_list
  - append_list

  其中nil没有，用来处理空的文法

  single有一个节点，处理single的文法，例如expression_list: expression

  append有两个节点，处理多个的文法，例如expression_list: expression expression_list

  

# 20200628

写完了PA3.

![PA3](https://weekly-assignment-1302348848.cos.ap-shanghai.myqcloud.com/week5/PA3.png)

一些细节要注意：

- 算符优先级（定义先后）
- error
- 逗号，分号等

# 20200629

今天看了文法制导翻译的一些基本概念，核心思路就是怎么根据abstract syntax tree去递归或者非递归地计算各个节点的semantic value/type.

下面有一些定义：

## 依赖属性

一个节点的属性如果依赖于子节点或者本身，就是综合属性，如果依赖于兄弟节点和父节点，就是继承属性。

不是所有依赖属性图（根据AST和依赖属性定义的图）都能通过递归/非递归去推导，因为可能存在循环依赖的情况，因此要给出一些限制来避免它：

## S属性

一个依赖图所有节点都是综合属性，那就是S属性的，显然S属性没有循环依赖。

## L属性

一个产生式所关联的各个属性间，依赖图只能从左到右，不能从右到左（避免子节点继承属性，父节点综合属性的循环依赖）

## 翻译过程

翻译过程可以在parse的同时进行，也非常相像，非递归翻译通过扩展语法分析栈，结合一些操作进行，只需要注意一下综合属性和继承属性处理方式的不同就行了（综合属性出现在syntax后，继承属性出自syntax前）

（p.s.哈工大这里就和cs143讲的地方不一样了，cs143从parser过后就开始讲semantic analysis了)

## Semantic Analysis

semantic analysis用来处理lexer和parser无法找出来的错误，这部分的正确与错误就完全取决于语言本身的特性了，比如静态语言要求一个变量在未声明前就不能使用，这个错误只有semantic analysis可以检查出来。

以下semantic analysis做的check摘自cs143的lecture note：

1. All identifiers are declared 
2. Types 
3. Inheritance relationships 
4. Classes defined only once 
5. Methods in a class defined only once 
6. Reserved identifiers are not misused 
7. And others . . . 

此外，semantic analysis还需要做scope的活，例如一个c++的例子：

```c++
#include<iostream>
int main(){
	int a = 1;
	{
		int a = 2;
		std::cout << a <<std::endl;
	}
	std::cout << a <<std::endl;
	return 0;
}
```

cpp中就通过curly brace来表示变量的scope。

### Symbol Table

Symbol Table是一种数据结构，用来保存标识符所绑定的当前值，如果用把symbol  table做成一个stack，就可以实现静态的scope（搜索identifier的binding时从栈顶往下，匹配到就结束，这样一个scope中定义的变量就能被先匹配到，等这个scope结束后把它出栈，就回到了原来的scope）。

cs143的lecture note给出了（关于COOL的）symbol table的method：

- enter_scope() start a new nested scope 
- find_symbol(x) finds current x (or null) 
- add_symbol(x) add a symbol x to the table
- check_scope(x) true if x defined in current scope
- exit_scope() exit current scope  

# 20200630

今天主要看了Inference Rule，一种拿来做semantic analysis的形式化推导语言，主要是这样的notation：

$ \frac{⊢ Hypothesis … ⊢ Hypothesis}{⊢ Conclusion}$

  利用这个就能做一些semantic的推导了，例如：

$\frac{⊢ a: Int, ⊢ b: Int}{⊢a + b: Int}$

或者一些更复杂的case，在有type environment的情况下：

 $\frac{O ⊢ e_0: T_0, O[T/x] ⊢ e_1: T_1, T_0 \leq T}{O ⊢ let x:T <- e_0 in e_1 : T_1}  $

在implement type system时，就可以按照这种规则递归进行：

```
TypeCheck(Environment, e1 + e2) = 
{ 
T1 = TypeCheck(Environment, e1); 
T2 = TypeCheck(Environment, e2); 
Check T1 == T2 == Int; return Int; 
} 
```

